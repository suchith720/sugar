{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e650b19-1e19-4f80-afdf-052ce81b600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 04_download-wikipedia-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ce8e5-88c1-4422-b142-2086b3e47a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3356d9-f403-4c57-afa1-91b83a2b6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests, os, re, argparse, multiprocessing as mp, ssl\n",
    "from timeit import default_timer as timer\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2960ce91-4265-4923-908d-d2f0f5101ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_file(url, cache_dir):\n",
    "    fn = f'{cache_dir}/{os.path.basename(url)}'\n",
    "    urlretrieve(url, filename=fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14e757-5b9a-4b34-9da7-e85abc6d5abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _full_condition(o):\n",
    "    if 'pages-articles-multistream' in o and '.xml-p' not in o and '.txt-p' not in o: return True\n",
    "    else: return False\n",
    "\n",
    "def _parts_condition(o):\n",
    "    if 'pages-articles-multistream' in o and ('.xml-p' in o or '.txt-p' in o): return True\n",
    "    else: return False\n",
    "\n",
    "def _abstract_condition(o):\n",
    "    if '-abstract.xml.gz' in o: return True\n",
    "    else: return False\n",
    "\n",
    "def get_condition(dtype):\n",
    "    if dtype == 'full': \n",
    "        return _full_condition\n",
    "    elif dtype == 'parts': \n",
    "        return _parts_condition\n",
    "    elif dtype == 'abstract': \n",
    "        return _abstract_condition\n",
    "    else: \n",
    "        raise ValueError(f'Invalid condition type: {dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c5593-752c-4a37-a7dd-5d8ccc2f65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_wikipedia_dataset(data_url, cache_dir, condition_type):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    html = requests.get(data_url).text\n",
    "    soup_handler = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    all_files = [file.text.split(' ')[0] for file in soup_handler.find_all('li', {'class': 'file'})]\n",
    "    condition = get_condition(condition_type)\n",
    "    urls = [f'{data_url}/{o}' for o in all_files if condition(o)]\n",
    "    print(f'Total number of file: {len(urls)}')\n",
    "\n",
    "    for url in tqdm(urls): download_file(url, cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec17be-08c9-46ed-8646-1d25fb8d4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_url', type=str, default='https://dumps.wikimedia.org/')\n",
    "    parser.add_argument('--condition_type', type=str, default='full')\n",
    "    parser.add_argument('--project', type=str, required=True)\n",
    "    parser.add_argument('--date', type=str, required=True)\n",
    "    parser.add_argument('--cache_dir', type=str, required=True)\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = timer()\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    data_url = f'{args.data_url}/{args.project}/{args.date}'\n",
    "    download_wikipedia_dataset(data_url, args.cache_dir, args.condition_type)\n",
    "\n",
    "    end_time = timer()\n",
    "    print(f'Time elapsed: {end_time-start_time:.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160b794-1ae3-4af1-87c7-5ab039582bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
