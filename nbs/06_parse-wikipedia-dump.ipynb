{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1134f93-6ce7-44a6-bb4e-93e1ce94bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 06_parse-wikipedia-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2aa020c-a66d-4e9b-bd1e-ebb963bbcca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c111337-4ead-4276-a799-077deeaf46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, bz2, queue, wikitextparser, multiprocessing, xml.sax, io, argparse, gzip, mwparserfromhell\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12279b45-d43e-4f8b-960f-1e462cc9aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_index_file(index_source, persist=True):\n",
    "    index_offsets_persisted = index_source + \".offsets\"\n",
    "\n",
    "    if os.path.exists(index_offsets_persisted):\n",
    "        try:\n",
    "            index_filehandle = open(index_offsets_persisted, \"r\")\n",
    "            offset_strings = index_filehandle.readlines()\n",
    "            sorted_offset_strings = [int(offset) for offset in offset_strings]\n",
    "            return sorted_offset_strings\n",
    "        finally:\n",
    "            index_filehandle.close()\n",
    "\n",
    "    else:\n",
    "        stream_offsets = set()\n",
    "        try:\n",
    "            index_filehandle = bz2.BZ2File(index_source)\n",
    "\n",
    "            last_offset = -1\n",
    "            for line in index_filehandle:\n",
    "                offset = int(line.decode(\"utf-8\").split(\":\")[0])\n",
    "                if offset != last_offset:\n",
    "                    stream_offsets.add(offset)\n",
    "                    last_offset = offset\n",
    "        finally:\n",
    "            index_filehandle.close()\n",
    "\n",
    "        sorted_stream_offsets = sorted(stream_offsets)\n",
    "\n",
    "        if persist:\n",
    "            try:\n",
    "                offset_output_filehandle = open(index_offsets_persisted, \"w\")\n",
    "                sorter_stream_offset_strings = [str(offset) for offset in sorted_stream_offsets]\n",
    "                sorter_stream_offset_string = '\\n'.join(sorter_stream_offset_strings)\n",
    "\n",
    "                offset_output_filehandle.write(sorter_stream_offset_string)\n",
    "            finally:\n",
    "                offset_output_filehandle.close()\n",
    "\n",
    "        return sorted_stream_offsets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26419f-9cb8-4174-ada1-52e825b41192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_worker(work_queue, work_queue_lock, articles_source, cache_dir, proc_type):\n",
    "    offsets_processed = 0\n",
    "    stream_filehandle = open(articles_source, \"rb\")\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                work_queue_lock.acquire()\n",
    "                stream_offset = work_queue.get(block=False)\n",
    "            finally:\n",
    "                work_queue_lock.release()\n",
    "\n",
    "            stream_filehandle.seek(stream_offset)\n",
    "            decompressor = bz2.BZ2Decompressor()\n",
    "\n",
    "            output = [b'<pages>']\n",
    "            while not decompressor.eof:\n",
    "                output.append(decompressor.decompress(stream_filehandle.read(65536)))\n",
    "            output.append(b'</pages>')\n",
    "\n",
    "            contents = b''.join(output)\n",
    "            process_stream_contents(contents, cache_dir, proc_type)\n",
    "            offsets_processed += 1\n",
    "    except queue.Empty:\n",
    "        return\n",
    "    finally:\n",
    "        print(\"Worker process shutting down after processing {} offsets\".format(offsets_processed))\n",
    "        stream_filehandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9216dbc3-0cc0-4b4f-8cf1-8cee4023e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XMLSAXParser(xml.sax.ContentHandler):\n",
    "    def __init__(self, cache_dir, proc_type):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cache_dir, self.proc_func = cache_dir, get_proc_func(cache_dir, proc_type)\n",
    "\n",
    "        self.read_stack = []\n",
    "        self.page_id = None\n",
    "        self.page_title = None\n",
    "        self.page_redirect = None\n",
    "        self.page_ns = None\n",
    "        self.page_content = None\n",
    "\n",
    "        self.page_count = 0\n",
    "        self.in_page = False\n",
    "\n",
    "    def startElement(self, tag_name, attributes):\n",
    "\n",
    "        self.text_aggregate = []\n",
    "\n",
    "        if tag_name == \"page\":\n",
    "            self.page_redirect = None\n",
    "            self.page_title = None\n",
    "            self.page_id = None\n",
    "            self.page_ns = None\n",
    "            self.page_content = None\n",
    "            self.in_page = True\n",
    "        else:\n",
    "            if (tag_name == \"redirect\") and (self.read_stack[-1] == \"page\"):\n",
    "                self.page_redirect = attributes[\"title\"]\n",
    "\n",
    "        self.read_stack.append(tag_name)\n",
    "\n",
    "    def endElement(self, tag_name):\n",
    "        if (len(self.read_stack) > 0) and (tag_name == self.read_stack[-1]):\n",
    "            del self.read_stack[-1]\n",
    "        else:\n",
    "            raise Exception(\"Tag ({}) does not match open tag ({}).\".format(tag_name, self.read_stack[-1]))\n",
    "\n",
    "        element_string = ''.join(self.text_aggregate)\n",
    "\n",
    "        if tag_name == \"page\":\n",
    "            self.in_page = False\n",
    "            # We have the whole page so do with it what you will\n",
    "            self.proc_func(self.cache_dir, self.page_id, self.page_ns, self.page_title, self.page_redirect, self.page_content)\n",
    "        else:\n",
    "            if self.in_page:\n",
    "                if self.read_stack[-1] == \"page\":\n",
    "                    if tag_name == \"title\":\n",
    "                        self.page_title = element_string\n",
    "                    elif (tag_name == \"id\") and self.read_stack[-1]:\n",
    "                        self.page_id = int(element_string)\n",
    "                    elif tag_name == \"ns\":\n",
    "                        self.page_ns = int(element_string)\n",
    "                elif self.read_stack[-1] == \"revision\":\n",
    "                    # the actual page contents exist as a revision\n",
    "                    if tag_name == \"text\":\n",
    "                        self.page_content = element_string\n",
    "\n",
    "    text_aggregate = []\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self.in_page:\n",
    "            self.text_aggregate.append(content)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137cebcf-e2e3-44b0-8307-6aa9ebe15243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_stream_contents(manyPages, cache_dir, proc_type):\n",
    "    reader = XMLSAXParser(cache_dir, proc_type)\n",
    "    try:\n",
    "        byte_stream = io.BytesIO(manyPages)\n",
    "        xml.sax.parse(byte_stream, reader)\n",
    "    finally:\n",
    "        byte_stream.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76289fa2-d93a-4793-98e9-0843b77d36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main(index_source, articles_source, cache_dir, proc_type):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    try:\n",
    "        sorted_stream_offsets = process_index_file(index_source)\n",
    "        if (sorted_stream_offsets is None) or (len(sorted_stream_offsets) < 1):\n",
    "            raise Exception(\"Index file unsuccessful\")\n",
    "\n",
    "        process_count = multiprocessing.cpu_count()//2\n",
    "\n",
    "        work_queue = multiprocessing.Queue()\n",
    "        work_queue_lock = multiprocessing.Lock()\n",
    "\n",
    "        [work_queue.put(x) for x in sorted_stream_offsets]\n",
    "\n",
    "        jobs = []\n",
    "\n",
    "        for i in range(process_count):\n",
    "            p = multiprocessing.Process(target=process_worker, args=(work_queue,work_queue_lock, articles_source, cache_dir, proc_type))\n",
    "            p.start()\n",
    "            jobs.append(p)\n",
    "\n",
    "        for j in jobs:\n",
    "            j.join()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5df4fc-e8c1-43c9-b878-d4a4b72f788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# this is a placeholder. Presumably you would do something more useful\n",
    "def extract_article_categories(cache_dir, page_id, page_ns, page_title, page_redirect, page_content):\n",
    "    if page_redirect is None and page_ns == 0:\n",
    "        page_parsed = wikitextparser.parse(page_content)\n",
    "\n",
    "        categories = [o.target for o in page_parsed.wikilinks if o.target is not None and o.target.startswith(\"Category:\")]\n",
    "        if len(categories):\n",
    "            with open(f'{cache_dir}/categories/{page_id}.txt', 'w') as file:\n",
    "                file.write(f'{page_id}->{page_title}\\n')\n",
    "                for o in categories: file.write(o + '\\n')\n",
    "            print(\"Parsed page {}-{}\".format(page_id, page_title))\n",
    "\n",
    "    elif page_redirect is not None and (page_ns == 0 or page_ns == 14) and len(page_title) and len(page_redirect):\n",
    "        fname = f'{cache_dir}/page_redirects/{page_id}.txt' if page_ns == 0 else f'{cache_dir}/category_redirects/{page_id}.txt'\n",
    "        with open(fname, 'w') as file:\n",
    "            file.write(f'{page_title}->{page_redirect}\\n')\n",
    "        print(f'{page_title}->{page_redirect}')\n",
    "\n",
    "def extract_article_text(cache_dir, page_id, page_ns, page_title, page_redirect, page_content):\n",
    "    if page_redirect is None and page_ns == 0:\n",
    "        page_parsed = mwparserfromhell.parse(page_content)\n",
    "        text = page_parsed.strip_code()\n",
    "        if len(text):\n",
    "            with open(f'{cache_dir}/texts/{page_id}.txt', 'w') as file:\n",
    "                file.write(f'{page_id}->{page_title}\\n')\n",
    "                file.write(text + '\\n')\n",
    "            print(\"Parsed page {}-{}\".format(page_id, page_title))\n",
    "\n",
    "    elif page_redirect is not None and page_ns == 0 and len(page_title) and len(page_redirect):\n",
    "        fname = f'{cache_dir}/page_redirects/{page_id}.txt'\n",
    "        with open(fname, 'w') as file:\n",
    "            file.write(f'{page_title}->{page_redirect}\\n')\n",
    "        print(f'{page_title}->{page_redirect}')\n",
    "\n",
    "def extract_article_redirects(cache_dir, page_id, page_ns, page_title, page_redirect, page_content):\n",
    "    if page_redirect is not None and page_ns == 0:\n",
    "        fname = f'{cache_dir}/page_redirects/{page_id}.txt'\n",
    "        with open(fname, 'w') as file:\n",
    "            file.write(f'{page_title}->{page_redirect}\\n')\n",
    "        print(f'{page_title}->{page_redirect}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c224781-d3eb-42d9-985f-776572840930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_proc_func(cache_dir, proc_type):\n",
    "    if proc_type == 'page_category':\n",
    "        os.makedirs(f'{cache_dir}/categories/', exist_ok=True)\n",
    "        os.makedirs(f'{cache_dir}/page_redirects/', exist_ok=True)\n",
    "        os.makedirs(f'{cache_dir}/category_redirects/', exist_ok=True)\n",
    "        return extract_article_categories\n",
    "    elif proc_type == 'page_text':\n",
    "        os.makedirs(f'{cache_dir}/texts/', exist_ok=True)\n",
    "        os.makedirs(f'{cache_dir}/page_redirects/', exist_ok=True)\n",
    "        return extract_article_text\n",
    "    elif proc_type == 'page_redirects':\n",
    "        os.makedirs(f'{cache_dir}/page_redirects/', exist_ok=True)\n",
    "        return extract_article_redirects\n",
    "    else: raise ValueError(f'Invalid function type: {proc_type}')\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--articles_source', type=str, required=True)\n",
    "    parser.add_argument('--cache_dir', type=str, required=True)\n",
    "    parser.add_argument('--proc_type', type=str, required=True)\n",
    "    parser.add_argument('--index_source', type=str, default=None)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a9545-feff-413f-974a-6d082c8ccf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = timer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
