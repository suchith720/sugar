{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1134f93-6ce7-44a6-bb4e-93e1ce94bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 06_parse-wikipedia-dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa020c-a66d-4e9b-bd1e-ebb963bbcca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c111337-4ead-4276-a799-077deeaf46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, bz2, queue, wikitextparser, multiprocessing, xml.sax, io, argparse, gzip, mwparserfromhell, pickle, joblib\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc46c4-6aff-4d61-9201-a27db3cdd11d",
   "metadata": {},
   "source": [
    "## `Index file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12279b45-d43e-4f8b-960f-1e462cc9aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_index_file(index_source, persist=True):\n",
    "    index_offsets_persisted = index_source + \".offsets\"\n",
    "\n",
    "    if os.path.exists(index_offsets_persisted):\n",
    "        with open(index_offsets_persisted, \"r\") as file: \n",
    "            offset_strings = file.readlines()\n",
    "            sorted_offset_strings = [int(offset) for offset in offset_strings]\n",
    "            return sorted_offset_strings\n",
    "    else:\n",
    "        stream_offsets = set()\n",
    "        with bz2.BZ2File(index_source) as file:\n",
    "            last_offset = -1\n",
    "            for line in file:\n",
    "                offset = int(line.decode(\"utf-8\").split(\":\")[0])\n",
    "                if offset != last_offset:\n",
    "                    stream_offsets.add(offset)\n",
    "                    last_offset = offset\n",
    "                    \n",
    "        sorted_stream_offsets = sorted(stream_offsets)\n",
    "        if persist:\n",
    "            with open(index_offsets_persisted, \"w\") as file:\n",
    "                sorter_stream_offset_strings = [str(offset) for offset in sorted_stream_offsets]\n",
    "                sorter_stream_offset_string = '\\n'.join(sorter_stream_offset_strings)\n",
    "                file.write(sorter_stream_offset_string)\n",
    "                \n",
    "        return sorted_stream_offsets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2a941-a08d-483e-a99b-390759aefe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/scratch/datasets/raw_data/' \n",
    "index_file = f'{data_dir}/enwiki-20250123-pages-articles-multistream-index.txt.bz2'\n",
    "\n",
    "offsets = process_index_file(index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249844f5-8fa9-4055-8401-0081134c4c32",
   "metadata": {},
   "source": [
    "## `Worker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860d7b4-8b77-4cde-8792-1f15fa38eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_worker(worker_queue, worker_outputs, articles_source):\n",
    "    offsets_processed = 0\n",
    "    stream_filehandle = open(articles_source, \"rb\")\n",
    "    try:\n",
    "        while True:\n",
    "            stream_offset = worker_queue.get()\n",
    "            if stream_offset is None:\n",
    "                worker_outputs.put(None)\n",
    "                break\n",
    "            \n",
    "            stream_filehandle.seek(stream_offset)\n",
    "            decompressor = bz2.BZ2Decompressor()\n",
    "\n",
    "            output = [b'<pages>']\n",
    "            while not decompressor.eof: output.append(decompressor.decompress(stream_filehandle.read(65536)))\n",
    "            output.append(b'</pages>')\n",
    "\n",
    "            contents = b''.join(output)\n",
    "\n",
    "            # actual task\n",
    "            page_info, redirect_info = process_stream_contents(contents)\n",
    "            page_info = extract_article_info(page_info)\n",
    "            \n",
    "            offsets_processed += 1\n",
    "            worker_outputs.put((page_info, redirect_info))\n",
    "    finally:\n",
    "        # print(f\"Worker process shutting down after processing {offsets_processed} offsets\")\n",
    "        stream_filehandle.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26419f-9cb8-4174-ada1-52e825b41192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_worker(work_queue, articles_source):\n",
    "    offsets_processed = 0\n",
    "    stream_filehandle = open(articles_source, \"rb\")\n",
    "    page_info, redirect_info = dict(), dict()\n",
    "    try:\n",
    "        while True:\n",
    "            stream_offset = work_queue.get()\n",
    "            if stream_offset is None: break\n",
    "\n",
    "            stream_filehandle.seek(stream_offset)\n",
    "            decompressor = bz2.BZ2Decompressor()\n",
    "\n",
    "            output = [b'<pages>']\n",
    "            while not decompressor.eof: output.append(decompressor.decompress(stream_filehandle.read(65536)))\n",
    "            output.append(b'</pages>')\n",
    "\n",
    "            contents = b''.join(output)\n",
    "            offsets_processed += 1\n",
    "            # return contents\n",
    "            \n",
    "            p_info, r_info = process_stream_contents(contents)\n",
    "            page_info.update(p_info)\n",
    "            redirect_info.update(r_info)\n",
    "            \n",
    "        return page_info, redirect_info\n",
    "            \n",
    "    except queue.Empty:\n",
    "        return\n",
    "        \n",
    "    finally:\n",
    "        print(f\"Worker process shutting down after processing {offsets_processed} offsets\")\n",
    "        stream_filehandle.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceeaff1-12ea-44aa-8117-37098a16d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb554e3-6837-4951-916f-7cd26d7b4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/scratch/datasets/raw_data/' \n",
    "article_file = f'{data_dir}/enwiki-20250123-pages-articles-multistream.xml.bz2'\n",
    "\n",
    "q = multiprocessing.Queue()\n",
    "for x in offsets[:10]: q.put(x)\n",
    "q.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd0345-be2b-4a91-bced-cd8f75b01cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker process shutting down after processing 10 offsets\n"
     ]
    }
   ],
   "source": [
    "contents = process_worker(q, article_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bba811-9039-4aa4-8c98-298b5847e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(contents)\n",
    "pages = soup.find_all('page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20c0a3-44dd-4048-b4f4-14461950dbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<page>\n",
       "<title>AmoeboidTaxa</title>\n",
       "<ns>0</ns>\n",
       "<id>24</id>\n",
       "<redirect title=\"Amoeba\"></redirect>\n",
       "<revision>\n",
       "<id>783865319</id>\n",
       "<parentid>627604809</parentid>\n",
       "<timestamp>2017-06-05T04:19:55Z</timestamp>\n",
       "<contributor>\n",
       "<username>Tom.Reding</username>\n",
       "<id>9784415</id>\n",
       "</contributor>\n",
       "<minor></minor>\n",
       "<comment>+{{Redirect category shell}} using [[Project:AWB|AWB]]</comment>\n",
       "<origin>783865319</origin>\n",
       "<model>wikitext</model>\n",
       "<format>text/x-wiki</format>\n",
       "<text bytes=\"74\" sha1=\"ehl9qk3qz207n09xw0qj0wargvzjh4s\" xml:space=\"preserve\">#REDIRECT [[Amoeba]]\n",
       "\n",
       "{{Redirect category shell|1=\n",
       "{{R from CamelCase}}\n",
       "}}</text>\n",
       "<sha1>ehl9qk3qz207n09xw0qj0wargvzjh4s</sha1>\n",
       "</revision>\n",
       "</page>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629e496-a5c0-4fad-807a-619df8a0bfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d2973-e1b7-4834-9908-b5f6c1a80797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker process shutting down after processing 10 offsets\n"
     ]
    }
   ],
   "source": [
    "page_info, redirect_info = process_worker(q, article_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde81f35-5721-425d-8917-f0820f86d90f",
   "metadata": {},
   "source": [
    "## `Collect pages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137cebcf-e2e3-44b0-8307-6aa9ebe15243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_stream_contents(manyPages):\n",
    "    reader = XMLSAXParser()\n",
    "    with io.BytesIO(manyPages) as byte_stream:\n",
    "        xml.sax.parse(byte_stream, reader)\n",
    "    return reader.page_info, reader.redirect_info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216dbc3-0cc0-4b4f-8cf1-8cee4023e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class XMLSAXParser(xml.sax.ContentHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.read_stack = []\n",
    "        self.page_count = 0\n",
    "        \n",
    "        self.page_id, self.page_title, self.page_redirect = None, None, None\n",
    "        self.page_ns, self.page_content = None, None\n",
    "        \n",
    "        self.in_page = False\n",
    "\n",
    "        self.page_info, self.redirect_info = dict(), dict()\n",
    "\n",
    "    def startElement(self, tag_name, attributes):\n",
    "        self.text_aggregate = []\n",
    "\n",
    "        if tag_name == \"page\":\n",
    "            self.page_redirect, self.page_title, self.page_id = None, None, None\n",
    "            self.page_ns, self.page_content, self.in_page = None, None, True\n",
    "        else:\n",
    "            if (tag_name == \"redirect\") and (self.read_stack[-1] == \"page\"):\n",
    "                self.page_redirect = attributes[\"title\"]\n",
    "\n",
    "        self.read_stack.append(tag_name)\n",
    "\n",
    "    def endElement(self, tag_name):\n",
    "        if (len(self.read_stack) > 0) and (tag_name == self.read_stack[-1]):\n",
    "            del self.read_stack[-1]\n",
    "        else:\n",
    "            raise Exception(\"Tag ({}) does not match open tag ({}).\".format(tag_name, self.read_stack[-1]))\n",
    "\n",
    "        element_string = ''.join(self.text_aggregate)\n",
    "\n",
    "        if tag_name == \"page\":\n",
    "            self.in_page = False\n",
    "            if self.page_redirect is None: \n",
    "                self.page_info[self.page_id] = {'title': self.page_title, 'ns': self.page_ns, 'content': self.page_content}\n",
    "            else:\n",
    "                self.redirect_info[self.page_id] = {'title': self.page_title, 'redirect': self.page_redirect}\n",
    "        else:\n",
    "            if self.in_page:\n",
    "                if self.read_stack[-1] == \"page\":\n",
    "                    if tag_name == \"title\": self.page_title = element_string\n",
    "                    elif (tag_name == \"id\") and self.read_stack[-1]: self.page_id = int(element_string)\n",
    "                    elif tag_name == \"ns\": self.page_ns = int(element_string)\n",
    "                elif self.read_stack[-1] == \"revision\":\n",
    "                    if tag_name == \"text\": self.page_content = element_string\n",
    "                    \n",
    "    def characters(self, content):\n",
    "        if self.in_page: self.text_aggregate.append(content)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656af61-74d1-456a-bd47-f4336b08d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info, redirect_info = process_stream_contents(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47a991-159e-4af1-abcb-063714bc456c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(732, 268)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_info), len(redirect_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733ec8e-00f9-4f48-950f-c006c5907a46",
   "metadata": {},
   "source": [
    "## `Extract content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d05c57-5b9f-4812-89b0-513974ed0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_article_info(page_info):\n",
    "    processed_info = dict()\n",
    "    \n",
    "    for k,v in page_info.items():\n",
    "        if v['ns'] == 0: \n",
    "            page_parsed = wikitextparser.parse(v['content'])\n",
    "\n",
    "            page_category, page_seealso, page_hyperlinks, page_content = set(), set(), set(), ''\n",
    "            for section in page_parsed.sections:\n",
    "                used_section = False\n",
    "                \n",
    "                \"\"\" extract categories from the page \"\"\"\n",
    "                categories = [o.target for o in section.wikilinks if o.target is not None and o.target.startswith(\"Category:\")]\n",
    "                if len(categories): \n",
    "                    page_category.update(categories)\n",
    "                    used_section = True\n",
    "    \n",
    "                \"\"\" extract seealso links \"\"\"\n",
    "                if section.title is not None and section.title.strip().lower() in ['see also', 'seealso']:\n",
    "                    seealso = [o.target for o in section.wikilinks]\n",
    "                    if len(seealso): \n",
    "                        page_seealso.update(seealso)\n",
    "                        used_section = True\n",
    "    \n",
    "                \"\"\" add page title and content \"\"\"\n",
    "                if not used_section:\n",
    "                    page_content = page_content + \" \" + section.plain_text()\n",
    "                    \n",
    "                    hyperlinks = [o.target.split('#')[0] for o in section.wikilinks if o.target is not None and len(o.target.split('#')[0])]\n",
    "                    if len(hyperlinks): page_hyperlinks.update(hyperlinks)\n",
    "\n",
    "            if len(page_category): processed_info.setdefault(k, {}).update({'category': list(page_category)})\n",
    "            if len(page_seealso): processed_info.setdefault(k, {}).update({'see_also': list(page_seealso)})\n",
    "            if len(page_hyperlinks): processed_info.setdefault(k, {}).update({'hyper_link': list(page_hyperlinks)})\n",
    "                \n",
    "            if k in processed_info: processed_info[k].update({'title':v['title'], 'content': page_content})\n",
    "                \n",
    "    return processed_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9761d9c-46e4-411d-b14a-13447dcf31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_info = extract_article_info(page_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e940d0-5a50-45d6-b4f3-da1b80c773f6",
   "metadata": {},
   "source": [
    "## `Main function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76289fa2-d93a-4793-98e9-0843b77d36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main(index_source, articles_source):\n",
    "    try:\n",
    "        sorted_stream_offsets = process_index_file(index_source)\n",
    "        if (sorted_stream_offsets is None) or (len(sorted_stream_offsets) < 1): raise Exception(\"Index file unsuccessful.\")\n",
    "        \n",
    "        process_count = multiprocessing.cpu_count()//2\n",
    "        worker_queue, worker_outputs = multiprocessing.Queue(), multiprocessing.Queue()\n",
    "        for x in sorted_stream_offsets: worker_queue.put(x)\n",
    "        for _ in range(process_count): worker_queue.put(None)\n",
    "\n",
    "        jobs = []\n",
    "        for i in range(process_count):\n",
    "            p = multiprocessing.Process(target=process_worker, args=(worker_queue, worker_outputs, articles_source))\n",
    "            p.start(); jobs.append(p)\n",
    "            \n",
    "        num_proc_finished, worker_results = 0, []\n",
    "        with tqdm(total=len(sorted_stream_offsets)) as pbar:\n",
    "            while True:\n",
    "                output = worker_outputs.get()\n",
    "                if output is None: num_proc_finished += 1\n",
    "                else: \n",
    "                    worker_results.append(output)\n",
    "                    pbar.update(1)\n",
    "                if num_proc_finished == process_count: break\n",
    "            \n",
    "        for j in jobs: j.join()\n",
    "            \n",
    "        return worker_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d9194a-cb46-4492-9c54-67cd14f8445f",
   "metadata": {},
   "source": [
    "## `__main__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada5d12-6cd2-4dae-a336-873f4773220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def combine_info(info):\n",
    "    page_info, redirect_info = dict(), dict()\n",
    "    for p,r in tqdm(info):\n",
    "        page_info.update(p)\n",
    "        redirect_info.update(r)\n",
    "    return page_info, redirect_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c224781-d3eb-42d9-985f-776572840930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--articles_source', type=str, required=True)\n",
    "    parser.add_argument('--index_source', type=str, default=None)\n",
    "    parser.add_argument('--cache_dir', type=str, required=True)\n",
    "    return parser.parse_args()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a9545-feff-413f-974a-6d082c8ccf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = timer()\n",
    "    \n",
    "    args = parse_args()\n",
    "\n",
    "    print('Extracting data ...')\n",
    "    results = main(args.index_source, args.articles_source)\n",
    "\n",
    "    print('Combining data ...')\n",
    "    page_info, redirect_info = combine_info(results)\n",
    "\n",
    "    print('Saving data ...')\n",
    "    os.makedirs(args.cache_dir, exist_ok=True)\n",
    "    \n",
    "    page_file = f'{args.cache_dir}/page_info.joblib'\n",
    "    joblib.dump(page_info, page_file)\n",
    "\n",
    "    redirect_file = f'{args.cache_dir}/redirect_info.joblib'\n",
    "    joblib.dump(redirect_info, redirect_file)\n",
    "    \n",
    "    end_time = timer()\n",
    "    print(f'Total time taken: {end_time - start_time:.3f}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2986ab-e6d1-4839-8a3d-2a51112e52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/scratch/datasets/raw_data/'\n",
    "index_file = f'{data_dir}/enwiki-20250123-pages-articles-multistream-index.txt.bz2'\n",
    "article_file = f'{data_dir}/enwiki-20250123-pages-articles-multistream.xml.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d8e8a-859a-46cb-871a-58f585ff5e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:42<00:00,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "results = main(index_file, article_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e93588-05ed-4da0-9880-b45b59e5e5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/scai/phd/aiz218323/scratch/datasets/raw_data//enwiki-20250123-pages-articles-multistream-index.txt.bz2',\n",
       " '/home/scai/phd/aiz218323/scratch/datasets/raw_data//enwiki-20250123-pages-articles-multistream.xml.bz2')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_file, article_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52466a6-ad48-47b9-ba26-b9543946080a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
