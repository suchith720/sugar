# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_amazon-review-2018.ipynb.

# %% auto 0
__all__ = ['remove_invalid_lbl', 'remove_invalid_data', 'delete_rows', 'clean_dataset', 'construct_dataset', 'save_dataset',
           'parse_args', 'construct_meta_dataset', 'save_category', 'save_meta_dataset']

# %% ../nbs/01_amazon-review-2018.ipynb 4
import requests, os, gzip, json, scipy.sparse as sp, numpy as np, argparse, pandas as pd, multiprocessing as mp
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm.auto import tqdm

# %% ../nbs/01_amazon-review-2018.ipynb 12
def remove_invalid_lbl(item2col, item_info):
    invalid_lbl, valid_item2col = [], {}
    for o in item2col:
        if o in item_info: 
            valid_item2col[o] = len(valid_item2col)
        else: 
            invalid_lbl.append(item2col[o])
    return valid_item2col, invalid_lbl

def remove_invalid_data(item2row, matrix):
    invalid_data, valid_item2row = [], {}
    valid_data_mask = matrix.getnnz(axis=1) > 0
    
    for o,m in zip(item2row, valid_data_mask):
        if m: valid_item2row[o] = len(valid_item2row)
        else: invalid_data.append(item2row[o])
            
    return valid_item2row, invalid_data
    

# %% ../nbs/01_amazon-review-2018.ipynb 13
def delete_rows(matrix, invalid_idx):
    num_lbl = matrix.shape[1]
    
    prev_idx, num_deleted_items = 0, 0
    data, indices, indptr = [], [], []
    m_data, m_indices, m_indptr = matrix.data, matrix.indices, matrix.indptr
    
    for idx in invalid_idx:
        start, end = m_indptr[prev_idx], m_indptr[idx]

        if end > start:
            data.append(m_data[start:end])
            indices.append(m_indices[start:end])
        indptr.append(m_indptr[prev_idx:idx] - num_deleted_items)
        
        prev_idx = idx+1
        num_deleted_items += m_indptr[idx+1] - m_indptr[idx]

    start = m_indptr[prev_idx]
    data.append(m_data[start:])
    indices.append(m_indices[start:])
    indptr.append(m_indptr[prev_idx:] - num_deleted_items)

    data, indices, indptr = np.hstack(data), np.hstack(indices), np.hstack(indptr)
    num_data = len(indptr)-1
    
    return sp.csr_matrix((data, indices, indptr), shape=(num_data, num_lbl), dtype=matrix.dtype)
    

# %% ../nbs/01_amazon-review-2018.ipynb 14
def clean_dataset(matrix, item2row, item2col, item_info):
    valid_item2col, invalid_lbl = remove_invalid_lbl(item2col, item_info)
    valid_matrix_t = delete_rows(matrix.transpose().tocsr(), invalid_lbl)
    valid_matrix = valid_matrix_t.transpose().tocsr()

    valid_item2row, invalid_data = remove_invalid_data(item2row, valid_matrix)
    valid_matrix = delete_rows(valid_matrix, invalid_data)
    
    return valid_matrix, valid_item2row, valid_item2col
    

# %% ../nbs/01_amazon-review-2018.ipynb 17
def construct_dataset(files):
    item_info, item2row, item2col = {}, {}, {}
    data, indices, indptr = [], [], [0]

    for file_idx,fname in enumerate(files):
        with gzip.open(fname, 'rt', encoding='utf-8') as f:
            items = [json.loads(d) for d in f]

        progress_bar = None
        for item in items:
            if progress_bar is None:
                progress_bar = tqdm(total=len(items), unit='items', desc=f'File {file_idx+1}')
            progress_bar.update(1)
            
            identifier = item['asin'] 
            short_text = item['title'] if 'title' in item else None
            full_text = ''
            if 'description' in item:
                full_text += ''.join(item['description'])
            elif 'feature' in item:
                full_text += ''.join(item['feature'])

            category = item['category'] if 'category' in item else None
            also_view = item['also_view'] if 'also_view' in item else None
            brand = item['brand'] if 'brand' in item else None
            similar_item = item['similar_item'] if 'similar_item' in item else None
            
            if identifier and len(identifier) > 0 and short_text and len(short_text) > 0:
                if identifier not in item_info:
                    item_info[identifier] = {'short_text': short_text, 'full_text': full_text, 'category': category, 
                                             'also_view': also_view, 'brand': brand, 'similar_item': similar_item}
                    
                    if ('also_buy' in item) and (identifier not in item2row):
                        item2row.setdefault(identifier, len(item2row))
                        data.extend([1] * len(item['also_buy']))
                        indices.extend([item2col.setdefault(o, len(item2col)) for o in item['also_buy']])
                        indptr.append(len(indices))
                
    matrix = sp.csr_matrix((data, indices, indptr), dtype=np.float16)
    return item_info, item2row, item2col, matrix
        

# %% ../nbs/01_amazon-review-2018.ipynb 18
def save_dataset(output_dir, matrix, item2row, item2col, item_info):
    data_lbl = f'{output_dir}/data_lbl.npz'
    sp.save_npz(data_lbl, matrix)

    data_info = pd.DataFrame({'identifier':list(item2row), 'text':[item_info[o]['short_text'] for o in item2row]})
    data_info.to_csv(f'{output_dir}/data_info.csv', index=False)

    lbl_info = pd.DataFrame({'identifier':list(item2col), 'text':[item_info[o]['short_text'] for o in item2col]})
    lbl_info.to_csv(f'{output_dir}/lbl_info.csv', index=False)

    with open(f'{output_dir}/item_info.json', 'w') as f:
        json.dump(item_info, f)
    

# %% ../nbs/01_amazon-review-2018.ipynb 19
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_url', type=str, required=True)
    parser.add_argument('--cache_dir', type=str, required=True)
    parser.add_argument('--output_dir', type=str, required=True)
    return parser.parse_args()
    

# %% ../nbs/01_amazon-review-2018.ipynb 83
def construct_meta_dataset(item_info, item2data, item2lbl, metatag):
    item2meta = {}

    def get_metadata(item2xy):
        data, indices, indptr = [], [], [0]
        progress_bar = tqdm(total=len(item2xy), unit='items')
        for identifier in item2xy:
            if metatag in item_info[identifier]:
                meta = item_info[identifier][metatag]
                if meta is not None:
                    data.extend([1] * len(meta))
                    indices.extend([item2meta.setdefault(o, len(item2meta)) for o in meta])
            indptr.append(len(indices))
            progress_bar.update(1)
        r,c = len(item2xy), len(item2meta)
        return sp.csr_matrix((data, indices, indptr), shape=(r,c), dtype=np.float16)
        
    data_meta, lbl_meta = get_metadata(item2data), get_metadata(item2lbl)
    data_meta.resize(len(item2data),len(item2meta))
    
    return item2meta, data_meta, lbl_meta
    

# %% ../nbs/01_amazon-review-2018.ipynb 93
def save_category(output_dir, data_meta, lbl_meta, meta_info, meta_tag):
    sp.save_npz(f'{output_dir}/data_{meta_tag}.npz', data_meta)
    sp.save_npz(f'{output_dir}/lbl_{meta_tag}.npz', lbl_meta)

    meta_info = pd.DataFrame({'text':list(meta_info)})
    meta_info.to_csv(f'{output_dir}/{meta_tag}_info.csv', index=False)
    

# %% ../nbs/01_amazon-review-2018.ipynb 96
def save_meta_dataset(output_dir, data_meta, lbl_meta, meta_info, item_info, meta_tag):
    sp.save_npz(f'{output_dir}/data_{meta_tag}.npz', data_meta)
    sp.save_npz(f'{output_dir}/lbl_{meta_tag}.npz', lbl_meta)
    
    meta_info = pd.DataFrame({'identifier':list(meta_info), 'text':[item_info[o]['short_text'] for o in meta_info]})
    meta_info.to_csv(f'{output_dir}/{meta_tag}_info.csv', index=False)
    
