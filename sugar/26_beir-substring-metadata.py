# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/26_beir-substring-metadata.ipynb.

# %% auto 0
__all__ = ['load_gpt_outputs', 'get_label_metadata', 'get_label_substring_metadata', 'get_query_substring_metadata',
           'is_valid_output', 'convert_string_to_objects', 'save_data', 'proc_dataset', 'main']

# %% ../nbs/26_beir-substring-metadata.ipynb 2
import os, json, pandas as pd, ast, scipy.sparse as sp, json_repair, numpy as np, re, csv

from tqdm.auto import tqdm
from typing import Optional, Dict, List

HEAD_DATASETS = [
    "climate-fever",
    "dbpedia-entity",
    "msmarco",
    "fever",
    "hotpotqa",
    "nq",
]

TAIL_DATASETS = [
    # "arguana",
    # "fiqa",
    # "nfcorpus",
    # "quora",
    # "scidocs",
    # "scifact",
    # "webis-touche2020",
    # "trec-covid",
    # "cqadupstack/android",
    # "cqadupstack/english",
    # "cqadupstack/gaming",
    # "cqadupstack/gis",
    # "cqadupstack/mathematica",
    # "cqadupstack/physics",
    # "cqadupstack/programmers",
    # "cqadupstack/stats",
    # "cqadupstack/tex",
    # "cqadupstack/unix",
    # "cqadupstack/webmasters",
    # "cqadupstack/wordpress"
]

# %% ../nbs/26_beir-substring-metadata.ipynb 4
def load_gpt_outputs(fname:str):
    outputs = dict()
    with open(fname) as file:
        for line in file:
            content = json.loads(line)
            outputs[content["doc_id"]] = content["output"]
    return outputs
    

# %% ../nbs/26_beir-substring-metadata.ipynb 5
def get_label_metadata(outputs:Dict, labels:pd.DataFrame):
    return [outputs.get(i, {}) for i in labels["identifier"]]
    

# %% ../nbs/26_beir-substring-metadata.ipynb 6
def get_label_substring_metadata(outputs:Dict):
    # substring metadata

    def get_derived_key(o):
        key = None
        for k in o.keys():
            if "derived_phrases" in k: 
                key = k
                break
        return key

    def get_original_key(o):
        key = None
        for k in o.keys():
            if "original" in k or "substring" in k:
                key = k
                break
        return key
    
    phrases = dict()
    vocab, data, indices, indptr = dict(), [], [], [0]
    for output in tqdm(outputs):
        for o in output.get("substring", []):
            if len(o):
                original_key = get_original_key(o)
                idx = vocab.setdefault(str(o[original_key]), len(vocab))
                data.append(1)
                indices.append(idx)

                derived_key = get_derived_key(o)
                if derived_key is not None:
                    assert np.all([type(i) == str for i in o[derived_key]]), '"derived_phrases" should be list of string'
                    phrases.setdefault(idx, []).extend(o[derived_key])

        indptr.append(len(data))
        
    lbl_sub = sp.csr_matrix((data, indices, indptr))
    sub_info = pd.DataFrame([(v,k) for k,v in vocab.items()], columns=["identifier", "text"])

    # derived phrases metadata
    
    vocab, data, indices, indptr = dict(), [], [], [0]
    for i in range(sub_info.shape[0]):
        for p in phrases.get(i, []):
            idx = vocab.setdefault(p, len(vocab))
            data.append(1)
            indices.append(idx)
        indptr.append(len(data))

    sub_phs = sp.csr_matrix((data, indices, indptr))
    phs_info = pd.DataFrame([(v,k) for k,v in vocab.items()], columns=["identifier", "text"])
    
    return lbl_sub, sub_info, sub_phs, phs_info
    

# %% ../nbs/26_beir-substring-metadata.ipynb 7
def get_query_substring_metadata(outputs:Dict, sub_info:pd.DataFrame):
    sub_vocab = {v:i for i,(k,v) in sub_info.iterrows()}
    
    # query-substring
    
    query, derived_queries = [], []
    data, indices, indptr = [], [], [0]
    for output in tqdm(outputs):
        for i,o in enumerate(output.get("queries", [])):
            query.append(o["primary_query"])
            idxs = [sub_vocab.setdefault(a, len(sub_vocab)) for a in o["answer"]]
            data.extend([1] * len(idxs))
            indices.extend(idxs)
            indptr.append(len(data))
            derived_queries.append(o["derived_queries"])
            
    data_sub = sp.csr_matrix((data, indices, indptr))
    data_info = pd.DataFrame([(k,v) for k,v in enumerate(query)], columns=["identifier", "text"])
    sub_info = pd.DataFrame([(v,k) for k,v in sub_vocab.items()], columns=["identifier", "text"])

    # query-derived_queries
    
    vocab, data, indices, indptr = dict(), [], [], [0]
    for queries in derived_queries:
        for q in queries:
            idx = vocab.setdefault(q, len(vocab))
            data.append(1)
            indices.append(idx)
        indptr.append(len(data))

    data_der = sp.csr_matrix((data, indices, indptr))
    der_info = pd.DataFrame([(v,k) for k,v in vocab.items()], columns=["identifier", "text"])

    return data_sub, data_info, sub_info, data_der, der_info
    

# %% ../nbs/26_beir-substring-metadata.ipynb 8
def is_valid_output(v:Dict):        
    if type(v) == dict:
        if "substring" in v:
            for o in v["substring"]:
                if len(o):
                    if not ("original_substring" in o and isinstance(o["original_substring"], str)):
                        return False
                        
                    if "derived_phrases" in o and isinstance(o["derived_phrases"], list):
                        if not np.all([isinstance(i, str) for i in o["derived_phrases"]]):
                            return False
                    else:
                        return False
        else:
            return False
    
        if "queries" in v:
            for o in v["queries"]:
                if not ("primary_query" in o and isinstance(o["primary_query"], str)):
                    return False
    
                if "derived_queries" in o and isinstance(o["derived_queries"], list):
                    if not np.all([isinstance(i, str) for i in o["derived_queries"]]):
                        return False
                else:
                    return False
    
                if "answer" in o and isinstance(o["answer"], list):
                    if not np.all([isinstance(i, str) for i in o["answer"]]):
                        return False
                else:
                    return False
        else:
            return False
    else:
        return False
        
    return True
    

# %% ../nbs/26_beir-substring-metadata.ipynb 9
def convert_string_to_objects(outputs:Dict):
    
    def get_line_number(pat:str, e:str):
        m = re.match(pat, str(e))
        return int(m.group(1)) - 1
        
    out, n_invalid = dict(), 0
    for k,v in tqdm(outputs.items()):
        error_flag, n_tries = True, 0

        if len(v) == 0: continue
        
        while error_flag:
            o = json_repair.loads(v)

            if "substrings" in o:
                o["substring"] = o.pop("substrings")
            
            if not is_valid_output(o):
                try:
                    out[k] = ast.literal_eval(v)
                    error_flag = False
                except Exception as e:
                    patterns = [
                        r"closing parenthesis '\)' does not match opening parenthesis '\[.*line (\d+)",
                        r"unterminated string literal \(detected at line \d+\).*line (\d+)\)",
                        r"invalid character '’' \(U\+2019\) .*line (\d+)\)",
                        r"invalid syntax .*line (\d+)\)",
                        r"closing parenthesis '\]' does not match opening parenthesis '\{' on line \d+ .*line (\d+)\)",
                    ]
                    
                    def func_1(line:str):
                        if line[-1] == "]":
                            line = line.replace(")", "")
                        else:
                            line = line.replace(")", "]")
                        return line

                    def func_2(line:str):
                        line = line.replace('”', '"').replace('“', '"')
                        if line[-1] == "]" and line[-2].strip() != '"':
                            line = line[:-1] + '"]'
                        if line[-2:] == "*/":
                            line = line[:-2] + '"]'
                        return line
                        
                    def func_3(line:str):
                        return line.replace("’", "'")

                    def func_4(line:str):
                        if line[-1] == ";": line = line[:-1]
                        return line

                    def func_5(line:str):
                        return line.replace("]", "} ]")

                    functions = [func_1, func_2, func_3, func_4, func_5]

                    n_tries += 1
                    if n_tries > 10:
                        n_invalid += 1
                        break
                        
                    for pat, func in zip(patterns, functions):
                        if re.match(pat, str(e)):
                            idx = get_line_number(pat, e)
                            lines = v.split("\n") 
                            lines[idx] = func(lines[idx])
                            v = "\n".join(lines)
                            break
                    else:
                        n_invalid += 1
                        break
                        
            else:
                out[k] = o
                error_flag = False

    if n_invalid > 0: 
        print(f"Number of invalid outputs: {n_invalid}")
        print(f"Invalid outputs: {n_invalid/len(outputs):.4f}")
        
    return out
    

# %% ../nbs/26_beir-substring-metadata.ipynb 10
def save_data(save_dir:str, dtype:str, data_sub:sp.csr_matrix, data_info:pd.DataFrame, sub_info:pd.DataFrame, 
              lbl_sub:sp.csr_matrix, sub_phs:sp.csr_matrix, phs_info:pd.DataFrame, data_der:sp.csr_matrix, 
              der_info:pd.DataFrame):

    os.makedirs(f"{save_dir}/raw_data/", exist_ok=True)
    short_hand = {"simple-query": "sq", "multihop-query": "mq"}

    assert dtype in short_hand, f"Invalid data-type: {dtype}."

    data_info.to_csv(f"{save_dir}/raw_data/{dtype}.raw.csv", index=False, quoting=csv.QUOTE_ALL, escapechar="\\")
    sub_info.to_csv(f"{save_dir}/raw_data/{short_hand[dtype]}-substring.raw.csv", index=False, quoting=csv.QUOTE_ALL, escapechar="\\")
    
    der_info.to_csv(f"{save_dir}/raw_data/{short_hand[dtype]}-derived-queries.raw.csv", index=False, quoting=csv.QUOTE_ALL, escapechar="\\")

    sp.save_npz(f"{save_dir}/{dtype}_{short_hand[dtype]}-substring.npz", data_sub)
    sp.save_npz(f"{save_dir}/{dtype}_{short_hand[dtype]}-derived-queries.npz", data_der)

    lbl_sub.resize((lbl_sub.shape[0], sub_info.shape[0]))
    sub_phs.resize((sub_info.shape[0], sub_phs.shape[1]))

    sp.save_npz(f"{save_dir}/lbl_{short_hand[dtype]}-substring.npz", lbl_sub)
    sp.save_npz(f"{save_dir}/{short_hand[dtype]}-substring_{short_hand[dtype]}-derived-phrases.npz", sub_phs)

    phs_info.to_csv(f"{save_dir}/raw_data/{short_hand[dtype]}-substring_{short_hand[dtype]}-derived-phrases.raw.csv", index=False, 
                    quoting=csv.QUOTE_ALL, escapechar="\\")
    

# %% ../nbs/26_beir-substring-metadata.ipynb 12
def proc_dataset(dataset:str, lbl_dir:str, data_dir:str, dset_type:str, save_dir:Optional[str]=None):

    lbl_file = f"{lbl_dir}/{dataset}/XC/raw_data/label.raw.csv"
    labels = pd.read_csv(lbl_file,  dtype={"identifier":str, "text":str})

    for dtype in ["simple", "multihop"]:
        print(f"Processing {dtype} queries ...", end="\n\n")

        out_file = f"{data_dir}/{dataset.replace('/', '-')}_{dtype}_label.jsonl"
        assert os.path.exists(out_file), f"File not found: {out_file}"

        outputs = load_gpt_outputs(out_file)

        keys = set(outputs.keys()).intersection(set(labels["identifier"].tolist()))

        if dset_type == "tail":
            assert len(keys)/labels.shape[0] > 0.9, "There are a lot of missing identifiers in the generated output."
        elif dset_type == "head":
            assert len(keys) > 80_000, "There are a lot of missing identifiers in the generated output."
        else:
            raise ValueError(f"Invalid dataset type: {dset_type}")
        
        outputs = convert_string_to_objects(outputs)
        outputs = get_label_metadata(outputs, labels)

        lbl_sub, sub_info, sub_phs, phs_info = get_label_substring_metadata(outputs)
        data_sub, data_info, sub_info, data_der, der_info = get_query_substring_metadata(outputs, sub_info)

        save_dir = f"{lbl_dir}/{dataset}/XC/document_substring/" if save_dir is None else save_dir
        save_data(save_dir, f"{dtype}-query", data_sub, data_info, sub_info, lbl_sub, sub_phs, phs_info, 
                  data_der, der_info)
        

# %% ../nbs/26_beir-substring-metadata.ipynb 13
def main(lbl_dir:str, data_dir:str, datasets:List, dset_type:str, save_dir:Optional[str]=None):
    for dataset in tqdm(datasets):
        print(dataset)
        print("=" * 50, end="\n\n")

        proc_dataset(dataset, lbl_dir, data_dir, dset_type, save_dir)

        print("=" * 50, end="\n\n")


# %% ../nbs/26_beir-substring-metadata.ipynb 14
if __name__ == "__main__":
    lbl_dir = "/data/datasets/beir/"

    data_dir = "/data/share/from_tanmay/tail_beir/"
    main(lbl_dir, data_dir, TAIL_DATASETS, "tail")

    data_dir = "/data/share/from_tanmay/head_beir/"
    main(lbl_dir, data_dir, HEAD_DATASETS, "head")
    
