# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/12_dataset-statistics.ipynb.

# %% auto 0
__all__ = ['METADATA_CODE', 'CODE_METADATA', 'PREFIX_METDATA', 'UpdatedDataset', 'Dataset', 'matrix_stats', 'main_dset_stats',
           'meta_dset_stats', 'dset_stats', 'trn_tst_stats', 'print_stats', 'print_dset_stats', 'TextDataset',
           'CompareDataset', 'get_filterer', 'save_labels', 'save_metadata', 'save_dataset', 'show_updated_dataset',
           'show_dataset']

# %% ../nbs/12_dataset-statistics.ipynb 3
import scipy.sparse as sp, re, xclib.data.data_utils as du, numpy as np, pandas as pd, os
from IPython.display import display
from torch.utils.data import Dataset
from termcolor import colored, COLORS

from .core import *
from xcai.data import *
from xcai.core import *

# %% ../nbs/12_dataset-statistics.ipynb 5
METADATA_CODE = {'category': 'cat', 'see_also': 'sal', 'hyper_link': 'hlk', 'videos': 'vid', 'images': 'img', 'entity_gpt': 'ent', 'entity_llama': 'ent', 
                 'entity': 'ent', 'canonical': 'can', 'entity_canonical_category': 'ecc', 'entity_canonical': 'enc'}

# %% ../nbs/12_dataset-statistics.ipynb 6
class UpdatedDataset:

    @staticmethod
    def load_data_info(data_dir, type, suffix=''):
        if len(suffix): suffix = f'.{suffix}'
        ids, txt = load_raw_file(f'{data_dir}/raw_data/{type}{suffix}.raw.csv')
        return {'identifier':ids, 'input_text':txt}

    @staticmethod
    def load_lbl_info(data_dir, x_prefix, y_prefix):
        ids, txt = load_raw_file(f'{data_dir}/raw_data/label.{x_prefix}-{y_prefix}.raw.csv')
        return {'identifier':ids, 'input_text':txt}

    @staticmethod
    def load_metadata_info(data_dir, metadata_type, x_prefix, y_prefix, z_prefix):
        ids, txt = load_raw_file(f'{data_dir}/raw_data/{metadata_type}.{x_prefix}-{y_prefix}-{z_prefix}.raw.csv')
        return {'identifier':ids, 'input_text':txt}

    @staticmethod
    def get_trn_tst_info(data_dir, suffix=''):
        trn_info = UpdatedDataset.load_data_info(data_dir, 'train', suffix)
        tst_info = UpdatedDataset.load_data_info(data_dir, 'test', suffix)
        return trn_info, tst_info

    @staticmethod
    def load_main_matrix(data_dir, x_prefix, y_prefix, type):
        if os.path.exists(f'{data_dir}/{type}_X_Y_{x_prefix}-{y_prefix}.npz'):
            mat = sp.load_npz(f'{data_dir}/{type}_X_Y_{x_prefix}-{y_prefix}.npz')
        else:
            mat = du.read_sparse_file(f'{data_dir}/{type}_X_Y_{x_prefix}-{y_prefix}.txt')
        return mat

    @staticmethod
    def get_labels(data_dir, x_prefix, y_prefix):
        trn_mat = UpdatedDataset.load_main_matrix(data_dir, x_prefix, y_prefix, 'trn')
        tst_mat = UpdatedDataset.load_main_matrix(data_dir, x_prefix, y_prefix, 'tst')
        
        lbl_info = UpdatedDataset.load_lbl_info(data_dir, x_prefix=x_prefix, y_prefix=y_prefix)
        
        return trn_mat, tst_mat, lbl_info

    @staticmethod
    def load_metadata_matrix(data_dir, x_prefix, y_prefix, z_prefix, main_type, metadata_type):
        if os.path.exists(f'{data_dir}/{metadata_type}_{main_type}_X_Y_{x_prefix}-{y_prefix}-{z_prefix}.npz'):
            mat = sp.load_npz(f'{data_dir}/{metadata_type}_{main_type}_X_Y_{x_prefix}-{y_prefix}-{z_prefix}.npz')
        else:
            mat = du.read_sparse_file(f'{data_dir}/{metadata_type}_{main_type}_X_Y_{x_prefix}-{y_prefix}-{z_prefix}.txt')
        return mat

    @staticmethod
    def get_metadata(data_dir, metadata_type, x_prefix, y_prefix, z_prefix):
        trn_mat = UpdatedDataset.load_metadata_matrix(data_dir, x_prefix, y_prefix, z_prefix, 'trn', metadata_type)
        tst_mat = UpdatedDataset.load_metadata_matrix(data_dir, x_prefix, y_prefix, z_prefix, 'tst', metadata_type)
        lbl_mat = UpdatedDataset.load_metadata_matrix(data_dir, x_prefix, y_prefix, z_prefix, 'lbl', metadata_type)
        
        meta_info = UpdatedDataset.load_metadata_info(data_dir, metadata_type, x_prefix, y_prefix, z_prefix)
        
        return trn_mat, tst_mat, lbl_mat, meta_info

    @staticmethod
    def load_datasets(data_dir, metadata_type, x_prefix, y_prefix, z_prefix):
        trn_info, tst_info = UpdatedDataset.get_trn_tst_info(data_dir, x_prefix)
        trn_mat, tst_mat, lbl_info = UpdatedDataset.get_labels(data_dir, x_prefix, y_prefix)
    
        main_trn_dset = MainXCDataset(trn_info, trn_mat, lbl_info)
        main_tst_dset = MainXCDataset(tst_info, tst_mat, lbl_info)
    
        trn_meta_mat, tst_meta_mat, lbl_meta_mat, meta_info = UpdatedDataset.get_metadata(data_dir, metadata_type, x_prefix, y_prefix, z_prefix)
    
        trn_meta_dset = MetaXCDataset(METADATA_CODE[metadata_type], trn_meta_mat, lbl_meta_mat, meta_info)
        tst_meta_dset = MetaXCDataset(METADATA_CODE[metadata_type], tst_meta_mat, lbl_meta_mat, meta_info)
    
        trn_dset = XCDataset(main_trn_dset, cat_meta=trn_meta_dset)
        tst_dset = XCDataset(main_tst_dset, cat_meta=tst_meta_dset)
    
        return trn_dset, tst_dset
        

# %% ../nbs/12_dataset-statistics.ipynb 8
class Dataset:

    @staticmethod
    def load_data_info(data_dir, type, suffix="", encoding='utf-8'):
        if len(suffix): suffix = f".{suffix}"
        fname = f'{data_dir}/raw_data/{type}{suffix}.raw'
        ids, txt = load_raw_file(fname+'.csv') if os.path.exists(fname+'.csv') else load_raw_file(fname+'.txt', encoding=encoding)
        return {'identifier':ids, 'input_text':txt}

    @staticmethod
    def load_lbl_info(data_dir, type, suffix="", encoding='utf-8', lbl_suffix=''):
        if len(suffix): suffix = f".{suffix}-{suffix}"
        elif len(lbl_suffix): suffix = f".{lbl_suffix}"
            
        fname = f'{data_dir}/raw_data/{type}{suffix}.raw'
        ids, txt = load_raw_file(fname+'.csv') if os.path.exists(fname+'.csv') else load_raw_file(fname+'.txt', encoding=encoding)
        return {'identifier':ids, 'input_text':txt}

    @staticmethod
    def load_metadata_info(data_dir, metadata_type, suffix="", encoding='utf-8'):
        if len(suffix): suffix = f".{suffix}-{suffix}-{suffix}"
        fname = f'{data_dir}/raw_data/{metadata_type}{suffix}.raw'
        ids, txt = load_raw_file(fname+'.csv') if os.path.exists(fname+'.csv') else load_raw_file(fname+'.txt', encoding=encoding)
        return {'identifier':ids, 'input_text':txt}

    @staticmethod
    def get_trn_tst_info(data_dir, suffix="", encoding='utf-8'):
        trn_info = Dataset.load_data_info(data_dir, 'train', suffix, encoding)
        tst_info = Dataset.load_data_info(data_dir, 'test', suffix, encoding)
        return trn_info, tst_info

    @staticmethod
    def get_labels(data_dir, suffix="", encoding='utf-8', lbl_suffix=''):
        s = f'_{lbl_suffix}' if len(lbl_suffix) else ''
        trn_mat = du.read_sparse_file(f'{data_dir}/trn_X_Y{s}.txt') if os.path.exists(f'{data_dir}/trn_X_Y{s}.txt') else sp.load_npz(f'{data_dir}/trn_X_Y{s}.npz')
        tst_mat = du.read_sparse_file(f'{data_dir}/tst_X_Y{s}.txt') if os.path.exists(f'{data_dir}/tst_X_Y{s}.txt') else sp.load_npz(f'{data_dir}/tst_X_Y{s}.npz')
            
        lbl_info = Dataset.load_lbl_info(data_dir, 'label', suffix, encoding, lbl_suffix)
        
        return trn_mat, tst_mat, lbl_info

    @staticmethod
    def get_metadata(data_dir, metadata_type, suffix="", encoding='utf-8', lbl_suffix=''):
        s = f'_{lbl_suffix}' if len(lbl_suffix) else ''
        trn_mat = du.read_sparse_file(f'{data_dir}/{metadata_type}_trn_X_Y.txt') if os.path.exists(f'{data_dir}/{metadata_type}_trn_X_Y.txt') else sp.load_npz(f'{data_dir}/{metadata_type}_trn_X_Y.npz')
        tst_mat = du.read_sparse_file(f'{data_dir}/{metadata_type}_tst_X_Y.txt') if os.path.exists(f'{data_dir}/{metadata_type}_tst_X_Y.txt') else sp.load_npz(f'{data_dir}/{metadata_type}_tst_X_Y.npz')
        
        lbl_mat = du.read_sparse_file(f'{data_dir}/{metadata_type}_lbl_X_Y{s}.txt') if os.path.exists(f'{data_dir}/{metadata_type}_lbl_X_Y{s}.txt') else sp.load_npz(f'{data_dir}/{metadata_type}_lbl_X_Y{s}.npz')
        
        meta_info = Dataset.load_metadata_info(data_dir, metadata_type, suffix, encoding)
        
        return trn_mat, tst_mat, lbl_mat, meta_info

    @staticmethod
    def load_datasets(data_dir, metadata_type, suffix="", encoding='utf-8', lbl_suffix=''):
        trn_info, tst_info = Dataset.get_trn_tst_info(data_dir, suffix, encoding)
        trn_mat, tst_mat, lbl_info = Dataset.get_labels(data_dir, suffix, encoding, lbl_suffix)
    
        main_trn_dset = MainXCDataset(trn_info, trn_mat, lbl_info)
        main_tst_dset = MainXCDataset(tst_info, tst_mat, lbl_info)
    
        trn_meta_mat, tst_meta_mat, lbl_meta_mat, meta_info = Dataset.get_metadata(data_dir, metadata_type, suffix, encoding, lbl_suffix)
    
        trn_meta_dset = {f'{METADATA_CODE[metadata_type]}_meta': MetaXCDataset(METADATA_CODE[metadata_type], trn_meta_mat, lbl_meta_mat, meta_info)}
        tst_meta_dset = {f'{METADATA_CODE[metadata_type]}_meta': MetaXCDataset(METADATA_CODE[metadata_type], tst_meta_mat, lbl_meta_mat, meta_info)}
    
        trn_dset = XCDataset(main_trn_dset, **trn_meta_dset)
        tst_dset = XCDataset(main_tst_dset, **tst_meta_dset)
    
        return trn_dset, tst_dset
        

# %% ../nbs/12_dataset-statistics.ipynb 10
CODE_METADATA = {'cat':'Category', 'sal':'Seealso', 'hlk':'Hyperlink', 'vid':'Videos', 'img':'Images', 
                 'ent':'Entity', 'can':'Canonical', 'ecc':'Entity Canonical Category', 'enc':'Entity Canonical'}

# %% ../nbs/12_dataset-statistics.ipynb 11
def matrix_stats(mat):
    n_dat = mat.shape[0]
    n_lbl = mat.shape[1]

    num_dat_lbl = mat.getnnz(axis=0)
    num_lbl_dat = mat.getnnz(axis=1)

    avg_dat_lbl = num_dat_lbl.mean()
    avg_lbl_dat = num_lbl_dat.mean()

    max_dat_lbl = num_dat_lbl.max()
    max_lbl_dat = num_lbl_dat.max()

    zro_dat_lbl = np.sum(num_dat_lbl == 0)
    zro_lbl_dat = np.sum(num_lbl_dat == 0)

    stats_dict = {
        f'# Entries' : n_dat,
        f'# Features': n_lbl,
        f'Avg. Entries per feature' : avg_dat_lbl,
        f'Avg. Feature per entry'   : avg_lbl_dat,
        f'Max. Entries per feature' : max_dat_lbl,
        f'Max. Feature per entry'   : max_lbl_dat,
        f'# Features without entry' : zro_dat_lbl,
        f'# Entries without feature': zro_lbl_dat,
    }
    return stats_dict
    

# %% ../nbs/12_dataset-statistics.ipynb 12
def main_dset_stats(dset):
    return matrix_stats(dset.data_lbl)
    

# %% ../nbs/12_dataset-statistics.ipynb 13
def meta_dset_stats(dset):
    dat_stats = matrix_stats(dset.data_meta)
    dat_stats['Dataset'] = 'Query'
    
    lbl_stats = matrix_stats(dset.lbl_meta)
    lbl_stats['Dataset'] = 'Label'

    return [dat_stats, lbl_stats]
    

# %% ../nbs/12_dataset-statistics.ipynb 14
def dset_stats(dset):
    stats = []
    
    main_stats = main_dset_stats(dset.data)
    main_stats['Dataset'] = 'Main'
    stats.append(main_stats)

    for o in dset.meta.values():
        meta_stats = meta_dset_stats(o)
        for s in meta_stats: s['Dataset'] = f'{s["Dataset"]} {CODE_METADATA[o.prefix]} Metadata'
        stats.extend(meta_stats)

    return stats
    

# %% ../nbs/12_dataset-statistics.ipynb 15
def trn_tst_stats(trn_dset, tst_dset):
    trn_stats = dset_stats(trn_dset)
    for o in trn_stats: o['Split'] = 'Train'

    tst_stats = dset_stats(tst_dset)
    for o in tst_stats: o['Split'] = 'Test'

    stats = trn_stats + tst_stats
    return stats
    

# %% ../nbs/12_dataset-statistics.ipynb 16
def print_stats(stats):
    df = pd.DataFrame(stats).set_index(['Split', 'Dataset'])
    with pd.option_context('display.precision', 2):
        display(df)
    

# %% ../nbs/12_dataset-statistics.ipynb 17
def print_dset_stats(trn_dset, tst_dset):
    stats = trn_tst_stats(trn_dset, tst_dset)
    print_stats(stats)
    

# %% ../nbs/12_dataset-statistics.ipynb 19
class TextDataset(Dataset):
    
    def __init__(self, dset, pattern='.*_text$'):
        self.dset, self.pattern = dset, pattern
        colors = list(COLORS.keys())
        self.colors = [colors[i] for i in np.random.permutation(len(colors))]
    
    def __getitem__(self, idx):
        o = self.dset[idx]
        return {k:v for k,v in o.items() if re.match(self.pattern, k)}

    def show(self, idxs):
        for idx in idxs:
            for i,(k,v) in enumerate(self[idx].items()):
                key = colored(k, self.colors[i], attrs=["reverse", "blink"])
                value = colored(f': {v}', self.colors[i])
                print(key, value)
            print()

    def get_head_data(self, topk=10):
        return np.argsort(self.dset.data.data_lbl.getnnz(axis=1))[:-topk:-1]

    def get_tail_data(self, topk=10):
        num = self.dset.data.data_lbl.getnnz(axis=1)
        idx = np.argsort(num)
        valid = (num > 0)[idx]
        return idx[valid][:topk]
        
    def dump_txt(self, fname, idxs):
        with open(fname, 'w') as file:
            for idx in idxs:
                for i,(k,v) in enumerate(self[idx].items()):
                    file.write(f'{k}: {v}\n')
                file.write('\n')
            
    def dump_csv(self, fname, idxs):
        df = pd.DataFrame([self[idx] for idx in idxs])
        df.to_csv(fname, index=False)

    def dump(self, fname, idxs):
        if fname.endswith('.txt'): 
            self.dump_txt(fname, idxs)
        elif fname.endswith('.csv'): 
            self.dump_csv(fname, idxs)
        else: 
            raise ValueError(f'Invalid file extension: {fname}')
        
    

# %% ../nbs/12_dataset-statistics.ipynb 21
class CompareDataset:

    def __init__(self, old_dset, new_dset):
        self.old_dset, self.new_dset = old_dset, new_dset
        colors = list(COLORS.keys())
        self.colors = [colors[i] for i in np.random.permutation(len(colors))]

    def data(self, topk=10, idx_type=''):
        assert self.old_dset.n_data == self.new_dset.n_data, f'Different number of datapoints in the two datasets.'

        old_lbl_dat = self.old_dset.data.data_lbl.getnnz(axis=1)
        new_lbl_dat = self.new_dset.data.data_lbl.getnnz(axis=1)

        if idx_type == 'max': idxs = np.argsort(new_lbl_dat - old_lbl_dat)[:-topk:-1]
        elif idx_type == 'min': idxs = np.argsort(new_lbl_dat - old_lbl_dat)[:topk]
        else: idxs = np.random.permutation(self.old_dset.n_data)[:topk]
        
        for idx in idxs:
            old_item = self.old_dset[idx]
            new_item = self.new_dset[idx]
            
            key = colored("data_input_text", self.colors[0], attrs=["reverse", "blink"])
            value = colored(f': {old_item["data_input_text"]}', self.colors[0])
            print(key, value)

            key = colored("old lbl2data_input_text", self.colors[1], attrs=["reverse", "blink"])
            value = colored(f': {old_item["lbl2data_input_text"]}', self.colors[1])
            print(key, value)

            key = colored("new lbl2data_input_text", self.colors[2], attrs=["reverse", "blink"])
            value = colored(f': {new_item["lbl2data_input_text"]}', self.colors[2])
            print(key, value)
            
            print()

    def metadata(self, metadata_type, data_type='data', topk=10, idx_type=''):
        assert self.old_dset.n_data == self.new_dset.n_data, f'Different number of datapoints in the two datasets.'

        if data_type == 'data':
            pattern = r"^(?!lbl).{3}2data_input_text"
            old_lbl_dat = self.old_dset.meta[metadata_type].data_meta.getnnz(axis=1)
            new_lbl_dat = self.new_dset.meta[metadata_type].data_meta.getnnz(axis=1)
        elif data_type == 'lbl':
            pattern = r"^.{3}2lbl2data_input_text"
            old_lbl_dat = self.old_dset.meta[metadata_type].lbl_meta.getnnz(axis=1)
            new_lbl_dat = self.new_dset.meta[metadata_type].lbl_meta.getnnz(axis=1)

        if idx_type == 'max': idxs = np.argsort(new_lbl_dat - old_lbl_dat)[:-topk:-1]
        elif idx_type == 'min': idxs = np.argsort(new_lbl_dat - old_lbl_dat)[:topk]
        else: idxs = np.random.permutation(self.old_dset.n_data)[:topk]
        
        for idx in idxs:
            old_item = self.old_dset[idx]
            new_item = self.new_dset[idx]

            key = colored("data_input_text", self.colors[0], attrs=["reverse", "blink"])
            value = colored(f': {old_item["data_input_text"]}', self.colors[0])
            print(key, value)

            key = colored("old lbl2data_input_text", self.colors[1], attrs=["reverse", "blink"])
            value = colored(f': {new_item["lbl2data_input_text"]}', self.colors[1])
            print(key, value)

            key = colored("new lbl2data_input_text", self.colors[2], attrs=["reverse", "blink"])
            value = colored(f': {new_item["lbl2data_input_text"]}', self.colors[2])
            print(key, value)

            ctr = 0
            for i,k in enumerate(old_item):
                if re.match(pattern, k):
                    key = colored(f"old {k}", self.colors[2*ctr+3], attrs=["reverse", "blink"])
                    value = colored(f': {old_item[k]}', self.colors[2*ctr+3])
                    print(key, value)
    
                    key = colored(f"new {k}", self.colors[2*ctr+4], attrs=["reverse", "blink"])
                    value = colored(f': {new_item[k]}', self.colors[2*ctr+4])
                    print(key, value)

                    ctr += 1
            
            print()
            

# %% ../nbs/12_dataset-statistics.ipynb 23
PREFIX_METDATA = {'cat': 'category', 'hlk': 'hyper_link', 'sal': 'see_also'}

# %% ../nbs/12_dataset-statistics.ipynb 24
def get_filterer(trn_ids, tst_ids, lbl_ids, trn_mat, tst_mat):
    trn_filterer, tst_filterer = Filterer.generate(trn_ids, tst_ids, lbl_ids, trn_mat, tst_mat)
    tst_mat = Filterer.apply(tst_mat, tst_filterer)
    return trn_filterer, tst_filterer, tst_mat
    

# %% ../nbs/12_dataset-statistics.ipynb 25
def save_labels(data_dir, trn_dset, tst_dset):
    os.makedirs(data_dir, exist_ok=True)

    if trn_dset.data.data_lbl_filterer is not None: np.savetxt(f'{data_dir}/filter_labels_train.txt', trn_dset.data.data_lbl_filterer)
    if tst_dset.data.data_lbl_filterer is not None: np.savetxt(f'{data_dir}/filter_labels_test.txt', tst_dset.data.data_lbl_filterer)

    sp.save_npz(f'{data_dir}/trn_X_Y.npz', trn_dset.data.data_lbl)
    sp.save_npz(f'{data_dir}/tst_X_Y.npz', tst_dset.data.data_lbl)

    os.makedirs(f'{data_dir}/raw_data', exist_ok=True)

    save_raw_file(f'{data_dir}/raw_data/train.raw.txt', trn_dset.data.data_info['identifier'], trn_dset.data.data_info['input_text'])
    save_raw_file(f'{data_dir}/raw_data/test.raw.txt', tst_dset.data.data_info['identifier'], tst_dset.data.data_info['input_text'])
    save_raw_file(f'{data_dir}/raw_data/label.raw.txt', trn_dset.data.lbl_info['identifier'], trn_dset.data.lbl_info['input_text'])
    

# %% ../nbs/12_dataset-statistics.ipynb 26
def save_metadata(data_dir, trn_dset, tst_dset):
    metadata_type = None
    
    for metadata in trn_dset.meta.keys():
        metadata_type = PREFIX_METDATA[trn_dset.meta[metadata].prefix]
        
        sp.save_npz(f'{data_dir}/{metadata_type}_trn_X_Y.npz', trn_dset.meta[metadata].data_meta)
        sp.save_npz(f'{data_dir}/{metadata_type}_tst_X_Y.npz', tst_dset.meta[metadata].data_meta)
        sp.save_npz(f'{data_dir}/{metadata_type}_lbl_X_Y.npz', trn_dset.meta[metadata].lbl_meta)
        
        save_raw_file(f'{data_dir}/raw_data/{metadata_type}.raw.txt', trn_dset.meta[metadata].meta_info['identifier'], trn_dset.meta[metadata].meta_info['input_text'])
    

# %% ../nbs/12_dataset-statistics.ipynb 27
def save_dataset(data_dir, trn_dset, tst_dset):
    trn_filterer, tst_filterer, tst_mat = get_filterer(trn_dset.data.data_info['identifier'], tst_dset.data.data_info['identifier'], 
                                                       trn_dset.data.lbl_info['identifier'], trn_dset.data.data_lbl, tst_dset.data.data_lbl)
    
    trn_dset.data.data_lbl_filterer = trn_filterer
    tst_dset.data.data_lbl_filterer = tst_filterer

    valid_idx = np.where(trn_dset.data.data_lbl.getnnz(axis=1) > 0)[0]
    trn_dset = trn_dset._getitems(valid_idx)

    valid_idx = np.where(tst_dset.data.data_lbl.getnnz(axis=1) > 0)[0]
    tst_dset = tst_dset._getitems(valid_idx)
    
    save_labels(data_dir, trn_dset, tst_dset)
    save_metadata(data_dir, trn_dset, tst_dset)

    return trn_dset, tst_dset
    

# %% ../nbs/12_dataset-statistics.ipynb 29
def show_updated_dataset(data_dir, metadata_type, x_prefix, y_prefix, z_prefix, idxs, use_trn=True):
    trn_dset, tst_dset = UpdatedDataset.load_datasets(data_dir, metadata_type, x_prefix, y_prefix, z_prefix)
    print_dset_stats(trn_dset, tst_dset)
    
    txt_dset = TextDataset(trn_dset if use_trn else tst_dset)
    txt_dset.show(idxs)
    
    return trn_dset, tst_dset

def show_dataset(data_dir, metadata_type, idxs, encoding='utf-8', lbl_suffix='', use_trn=True):
    trn_dset, tst_dset = Dataset.load_datasets(data_dir, metadata_type, encoding=encoding, lbl_suffix=lbl_suffix)
    print_dset_stats(trn_dset, tst_dset)
    
    txt_dset = TextDataset(trn_dset if use_trn else tst_dset)
    txt_dset.show(idxs)
    
    return trn_dset, tst_dset
    
